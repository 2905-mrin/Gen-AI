{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPh0gY9kM/3rDbAiNZfBOKZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2905-mrin/Gen-AI/blob/main/PES2UG23CS353_QUESTIONS_UNIT1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n"
      ],
      "metadata": {
        "id": "ec9L4-eapElu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    \"BERT\": \"bert-base-uncased\",\n",
        "    \"RoBERTa\": \"roberta-base\",\n",
        "    \"BART\": \"facebook/bart-base\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "ZzrOw4lSpJuX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 1"
      ],
      "metadata": {
        "id": "dfDMxsq2vsZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The future of artificial intelligence is\"\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nModel: {name}\")\n",
        "    try:\n",
        "        generator = pipeline(\"text-generation\", model=model)\n",
        "        output = generator(prompt, max_length=40)\n",
        "        print(output[0][\"generated_text\"])\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NsKVqZWpMDU",
        "outputId": "a35ad7a5-2835-4f75-c4e2-b2e0e6fbfd19"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model: BERT\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The future of artificial intelligence is................................................................................................................................................................................................................................................................\n",
            "\n",
            "Model: RoBERTa\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The future of artificial intelligence is\n",
            "\n",
            "Model: BART\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The future of artificial intelligence is nasal nasalinsk Download Download Metro Metroseversever observation Nem Sitting Download Sittingsc applies Sitting SittingolateedInSand Download Sitting separat Sitting Sitting ns nsovich separat separatovich Sitting Sitting050 ns Sitting Sitting Screw Sittingurous Sitting Sitting cleaners Sitting mism Sitting Royal ns mism Sitting cleanersurous Sittingolate Sitting Sitting Sitting lostendezendez SittingSand Sitting SittingCarter Sitting separat pouring Sitting separatCLASS Sitting Sitting separat blockade Sitting Sittingurous pouring Sitting Royal Enix Sitting ns Royal SittingWeapons Sitting Sitting joining Sitting ​ Sittingdidn Sitting Sitting ​Weapons Royal SittingOSEDurous Sittingurouscart hospital Sitting ​ ​CLASS separat separat ​reaching Alien Sitting hospital ​ ​ Augerence Sitting separatreaching Sitting Reverse Sitting joining Alien ​ ​ ​ nsurousographic ​ Aug separat ​ ​ joining ​ ​urous hospital Sitting ns ​ ​OSED ​ ​reaching ​ ​erence Aug ​ ​ Tokyoolate ​ ​ separat ​ static ​ ​Ton ​ ns ​ ns nsolatereaching ​ SPEC ​Tickets ​ ​Cryptolate ​reachingOSED ​xp ​ ​Revolution ​ ​ Fake ​ ​Michellemas ​ ​ Reversereaching Ever ​ Gods ​ ​mas ​reachingolateolate ​ Reverse ​CLASS ​ ​ cuff ​reachingurous ​ ​ Alien ​reaching retain ​ ​astern ​ ​olate ​ IGF ​ ​ ha ​reaching Freddie ​ ​ Disk ​ ​ Ever ​ ​ Lucia ​ ​\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothesis\n",
        "\n",
        "\n",
        "\n",
        "*   BERT The model produced repeated symbols (dots) and failed to generate meaningful text beyond the prompt because BERT is an encoder-only model and is not trained for autoregressive next-token generation.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   RoBERTa: The model stopped after the prompt and did not generate any continuation.RoBERTa is also encoder-only and lacks a decoding mechanism for text generation.\n",
        "\n",
        "*   BART: The model generated long, incoherent and gibberish text with repeated and unrelated tokens.BART is an encoder–decoder model, but it is not optimized as a causal language model for free-form text generation.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QEl3aZ3Su15T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 2\n",
        "\n"
      ],
      "metadata": {
        "id": "sr-UGS2aqYOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = {\n",
        "    \"BERT\": \"The goal of Generative AI is to [MASK] new content.\",\n",
        "    \"RoBERTa\": \"The goal of Generative AI is to <mask> new content.\",\n",
        "    \"BART\": \"The goal of Generative AI is to <mask> new content.\"\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nModel: {name}\")\n",
        "    try:\n",
        "        fill = pipeline(\"fill-mask\", model=model)\n",
        "        print(fill(sentences[name])[:3])\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2DwoHD8rO7m",
        "outputId": "13c8c119-2c53-4ff4-8ec8-91127da9f8d3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model: BERT\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'score': 0.5396937131881714, 'token': 3443, 'token_str': 'create', 'sequence': 'the goal of generative ai is to create new content.'}, {'score': 0.15575705468654633, 'token': 9699, 'token_str': 'generate', 'sequence': 'the goal of generative ai is to generate new content.'}, {'score': 0.05405480042099953, 'token': 3965, 'token_str': 'produce', 'sequence': 'the goal of generative ai is to produce new content.'}]\n",
            "\n",
            "Model: RoBERTa\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'score': 0.37113314867019653, 'token': 5368, 'token_str': ' generate', 'sequence': 'The goal of Generative AI is to generate new content.'}, {'score': 0.36771294474601746, 'token': 1045, 'token_str': ' create', 'sequence': 'The goal of Generative AI is to create new content.'}, {'score': 0.08351416140794754, 'token': 8286, 'token_str': ' discover', 'sequence': 'The goal of Generative AI is to discover new content.'}]\n",
            "\n",
            "Model: BART\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'score': 0.0746152326464653, 'token': 1045, 'token_str': ' create', 'sequence': 'The goal of Generative AI is to create new content.'}, {'score': 0.06571866571903229, 'token': 244, 'token_str': ' help', 'sequence': 'The goal of Generative AI is to help new content.'}, {'score': 0.060880016535520554, 'token': 694, 'token_str': ' provide', 'sequence': 'The goal of Generative AI is to provide new content.'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothesis\n",
        "\n",
        "\n",
        "\n",
        "*   BERT:Correctly predicted words such as create, generate, and produce with high confidence.BERT is trained using Masked Language Modeling (MLM), making it well-suited for predicting missing words.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   RoBERTa: Generated accurate and meaningful predictions like generate and create.RoBERTa is an optimized encoder-only model trained extensively on MLM, leading to strong performance.\n",
        "\n",
        "*  BART:Produced reasonable predictions but with lower confidence compared to BERT and RoBERTa.BART supports masking but is not primarily trained for MLM; its focus is sequence-to-sequence tasks.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lW6F9DEsv3ie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 3"
      ],
      "metadata": {
        "id": "iljKoPNEqZTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
        "question = \"What are the risks?\"\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nModel: {name}\")\n",
        "    try:\n",
        "        qa = pipeline(\"question-answering\", model=model)\n",
        "        print(qa(question=question, context=context))\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDD0FuuLrdJe",
        "outputId": "b8939f4f-b128-473e-9a21-11d91dcd3184"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model: BERT\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.00544058857485652, 'start': 72, 'end': 82, 'answer': 'deepfakes.'}\n",
            "\n",
            "Model: RoBERTa\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.008920054417103529, 'start': 0, 'end': 71, 'answer': 'Generative AI poses significant risks such as hallucinations, bias, and'}\n",
            "\n",
            "Model: BART\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.03488917648792267, 'start': 43, 'end': 45, 'answer': 'as'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothesis\n",
        "\n",
        "\n",
        "\n",
        "*   BERT:Returned a partial phrase from the context instead of a complete, well-formed answer.The model was not fine-tuned for QA; the QA head is randomly initialized, leading to unreliable span selection.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   RoBERTa: Produced a plausible but incomplete span from the context.Although encoder-based and good at understanding, it lacks task-specific QA fine-tuning.\n",
        "\n",
        "\n",
        "\n",
        "*   BART: Returned only a single keyword (deepfakes) instead of the full answer.BART can perform QA but requires fine-tuning; without it, answer extraction is inconsistent.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JtKEgo85wVAx"
      }
    }
  ]
}
